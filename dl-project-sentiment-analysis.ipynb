{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 7949045,
          "sourceType": "datasetVersion",
          "datasetId": 4674508
        }
      ],
      "dockerImageVersionId": 30674,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "notebookb5b18f2465",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SamhithVkk/dl-project/blob/main/dl-project-sentiment-analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = '515k-hotel-reviews-data-in-europe1:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F4674508%2F7949045%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240424%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240424T155817Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D422421ceb8f2659e01e32baca55a7733b33ed3784db5d4388ccb1ec860fb635998d6a71d4ec4f3c1223b2056d29899763550fe57ed0f74b82d126e035279132a0b1c0a966d9d1238f70dbc9f086c46e589071cc3472a34ea2f3df61a94125eb093b1df8310cae85fbea0998067304198bfddfb895018f3cba83e6e013bdfee9e976e98eb47ff6837dfeee8d5df4242ae5a3ee1765c2bd52604735028ea0314352a9a224ce7c56b9d7dfca4941e4ff632ef636cd1de4e838b48f9682e2285b5dac2f14a866a93e59915dd63ee8393d4c0e57de71648567684e8095b93cfb549db165176529f1cae425db1a87d42e1f595a4157797ab129d5e1b7d0ea4b9996af9'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "SO3hUzwV1IqM"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, SpatialDropout1D\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import string\n",
        "\n",
        "# Function to clean text\n",
        "def clean_text(text):\n",
        "    text = text.lower()  # Lowercase the text\n",
        "    text = ''.join([char for char in text if char not in string.punctuation])  # Remove punctuation\n",
        "    text = ''.join([char for char in text if not char.isdigit()])  # Remove digits\n",
        "    text = ' '.join(text.split())  # Remove extra whitespace\n",
        "    return text\n",
        "\n",
        "# Read the data\n",
        "reviews_df = pd.read_csv(\"/kaggle/input/515k-hotel-reviews-data-in-europe1/Hotel_Reviews.csv\")\n",
        "\n",
        "# Create the review text and label\n",
        "reviews_df[\"review\"] = reviews_df[\"Negative_Review\"] + reviews_df[\"Positive_Review\"]\n",
        "reviews_df[\"sentiment\"] = np.where(reviews_df[\"Reviewer_Score\"] < 5, 1, 0)  # 1 for negative, 0 for positive\n",
        "\n",
        "# Clean the text data\n",
        "reviews_df[\"review_clean\"] = reviews_df[\"review\"].apply(clean_text)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(reviews_df[\"review_clean\"], reviews_df[\"sentiment\"], test_size=0.2, random_state=42)\n",
        "\n",
        "# Tokenize the text data\n",
        "max_features = 10000  # Maximum number of words to keep based on frequency\n",
        "tokenizer = Tokenizer(num_words=max_features, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "# Pad the sequences\n",
        "maxlen = 100  # Maximum length of sequences\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=maxlen, padding='post')\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=maxlen, padding='post')\n",
        "\n",
        "# Define the model\n",
        "embedding_dim = 100\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=max_features, output_dim=embedding_dim),\n",
        "    SpatialDropout1D(0.2),\n",
        "    LSTM(100, dropout=0.2, recurrent_dropout=0.2),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Define early stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "epochs = 10\n",
        "batch_size = 64\n",
        "history = model.fit(X_train_pad, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test_pad, y_test), callbacks=[early_stopping])\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test_pad, y_test)\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-03-27T03:46:18.974399Z",
          "iopub.execute_input": "2024-03-27T03:46:18.975028Z",
          "iopub.status.idle": "2024-03-27T05:07:01.38239Z",
          "shell.execute_reply.started": "2024-03-27T03:46:18.975Z",
          "shell.execute_reply": "2024-03-27T05:07:01.3815Z"
        },
        "trusted": true,
        "id": "hrZc72QB1IqP",
        "outputId": "19c189dc-cf32-4e44-a27b-e28d6d755f4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "2024-03-27 03:46:22.850577: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-27 03:46:22.850676: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-27 03:46:22.980223: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 1/10\n\u001b[1m6447/6447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m927s\u001b[0m 143ms/step - accuracy: 0.9572 - loss: 0.1609 - val_accuracy: 0.9641 - val_loss: 0.1017\nEpoch 2/10\n\u001b[1m6447/6447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m921s\u001b[0m 143ms/step - accuracy: 0.9651 - loss: 0.0959 - val_accuracy: 0.9648 - val_loss: 0.0967\nEpoch 3/10\n\u001b[1m6447/6447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m917s\u001b[0m 142ms/step - accuracy: 0.9671 - loss: 0.0893 - val_accuracy: 0.9650 - val_loss: 0.0986\nEpoch 4/10\n\u001b[1m6447/6447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m916s\u001b[0m 142ms/step - accuracy: 0.9695 - loss: 0.0820 - val_accuracy: 0.9646 - val_loss: 0.0980\nEpoch 5/10\n\u001b[1m6447/6447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m944s\u001b[0m 146ms/step - accuracy: 0.9703 - loss: 0.0805 - val_accuracy: 0.9640 - val_loss: 0.0993\n\u001b[1m3224/3224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 42ms/step - accuracy: 0.9647 - loss: 0.0966\nTest Loss: 0.0967472642660141\nTest Accuracy: 0.9648078680038452\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Input review\n",
        "input_review = \"This hotel is bad and it is so uncomfortable but it is good for the price they have given than other hotels\"\n",
        "\n",
        "# Clean the input review\n",
        "cleaned_review = clean_text(input_review)\n",
        "\n",
        "# Tokenize and pad the cleaned review text\n",
        "review_seq = tokenizer.texts_to_sequences([cleaned_review])\n",
        "review_pad = pad_sequences(review_seq, maxlen=maxlen, padding='post')\n",
        "\n",
        "# Predict sentiment\n",
        "prediction = model.predict(review_pad)[0][0]\n",
        "\n",
        "# Output result\n",
        "if prediction >= 0.5:\n",
        "    print(\"The review is predicted to be negative.\")\n",
        "else:\n",
        "    print(\"The review is predicted to be positive.\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-27T05:44:39.737523Z",
          "iopub.execute_input": "2024-03-27T05:44:39.737887Z",
          "iopub.status.idle": "2024-03-27T05:44:39.849595Z",
          "shell.execute_reply.started": "2024-03-27T05:44:39.737859Z",
          "shell.execute_reply": "2024-03-27T05:44:39.848698Z"
        },
        "trusted": true,
        "id": "eqwUdwrc1IqP",
        "outputId": "ea4541cc-b67c-409d-dd07-36d59e42ca37"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\nThe review is predicted to be negative.\n",
          "output_type": "stream"
        }
      ]
    }
  ]
}